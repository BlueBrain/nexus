# v1.4 To v1.5 Migration

As part of the v1.5 release, a lot of improvements on how resources are handled and on how events are stored behind the scene 

(To have a complete list of improvements, see here //TODO Insert link to release notes).

These improvements require to replay all existing events to a new keyspace in order to:
* Adopt the new event schema for every event ;
* Detect and report broken resources in the system ;
* Attempt to fix them when possible.

Depending on the number and the complexity of existing resources and schemas in the system, 
the migration may last several days (about XX,XXX events per hour //TODO Insert number) to complete.

Nonetheless, migration does not require to stop an interruption of service of Delta 1.4 to be performed.

During migration, Delta still provides endpoints compatible with Delta 1.4 so that tests can be performed by fetching migrated resources.
Note that the responses of the endpoints will reflect the point of time, migration reached.

**Please do not perform any write operations as they may have an impact on the ongoing migration.**

## Pre-requirements
As migration require having two coexisting keyspaces during migration, the following resources are recommended:
* Disk space //TODO give estimation
* CPU/RAM

## Steps

The migration steps are as follows:
1. Make a @link:[backup](https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupRestore.html){ open=new } of the original keyspace `delta_1_4` and perform a repair of the Cassandra nodes
   ```
   nodetool repair
   ```
2. Create a materialized view on the messages table on the original keyspace `delta_1_4`:

   ```
   CREATE MATERIALIZED VIEW {delta_1_4}.ordered_messages AS
     SELECT ser_manifest, ser_id, event
     FROM delta.messages
     WHERE timebucket is not null
     AND persistence_id is not null
     AND partition_nr is not null
     AND sequence_nr is not null
     AND timestamp is not null
     PRIMARY KEY(timebucket, timestamp, persistence_id, partition_nr, sequence_nr)
     WITH CLUSTERING ORDER BY (timestamp asc);
   ```
The materialized view will take some time to build depending on the number of events, we will come back to it later.

3. Create the new keyspace for `delta 1_5`:
   ```
   CREATE KEYSPACE IF NOT EXISTS delta_1_5
      WITH REPLICATION = { 'class' : 'SimpleStrategy','replication_factor':3 };

   CREATE KEYSPACE IF NOT EXISTS delta_1_5_snapshot
      WITH REPLICATION = { 'class' : 'SimpleStrategy','replication_factor':3 };

   CREATE TABLE IF NOT EXISTS delta_1_5.messages(
      persistence_id    text,
      partition_nr      bigint,
      sequence_nr       bigint,
      timestamp         timeuuid,
      timebucket        text,
      writer_uuid       text,
      ser_id            int,
      ser_manifest      text,
      event_manifest    text,
      event             blob,
      meta_ser_id       int,
      meta_ser_manifest text,
      meta              blob,
      tags              set<text>,
      PRIMARY KEY ((persistence_id, partition_nr), sequence_nr, timestamp))
      WITH gc_grace_seconds =864000
      AND compression = {'class': 'LZ4Compressor'}
      AND compaction = {
         'class' : 'SizeTieredCompactionStrategy',
         'enabled' : true,
         'tombstone_compaction_interval' : 86400,
         'tombstone_threshold' : 0.2,
         'unchecked_tombstone_compaction' : false,
         'bucket_high' : 1.5,
         'bucket_low' : 0.5,
         'max_threshold' : 32,
         'min_threshold' : 4,
         'min_sstable_size' : 50
      };

   CREATE TABLE IF NOT EXISTS delta_1_5.tag_views(
      tag_name            text,
      persistence_id      text,
      sequence_nr         bigint,
      timebucket          bigint,
      timestamp           timeuuid,
      tag_pid_sequence_nr bigint,
      writer_uuid         text,
      ser_id              int,
      ser_manifest        text,
      event_manifest      text,
      event               blob,
      meta_ser_id         int,
      meta_ser_manifest   text,
      meta                blob,
      PRIMARY KEY ((tag_name, timebucket), timestamp, persistence_id, tag_pid_sequence_nr))
      WITH gc_grace_seconds =864000
      AND compression = {'class': 'LZ4Compressor'}
      AND compaction = {
         'class' : 'SizeTieredCompactionStrategy',
         'enabled' : true,
         'tombstone_compaction_interval' : 86400,
         'tombstone_threshold' : 0.2,
         'unchecked_tombstone_compaction' : false,
         'bucket_high' : 1.5,
         'bucket_low' : 0.5,
         'max_threshold' : 32,
         'min_threshold' : 4,
         'min_sstable_size' : 50
      };
   
   CREATE TABLE IF NOT EXISTS delta_1_5.tag_write_progress(
      persistence_id      text,
      tag                 text,
      sequence_nr         bigint,
      tag_pid_sequence_nr bigint,
      offset              timeuuid,
      PRIMARY KEY (persistence_id, tag)
   );
   
   CREATE TABLE IF NOT EXISTS delta_1_5.tag_scanning(
      persistence_id text,
      sequence_nr    bigint,
      PRIMARY KEY (persistence_id)
   );
   
   CREATE TABLE IF NOT EXISTS delta_1_5.metadata(
      persistence_id text PRIMARY KEY,
      deleted_to     bigint,
      properties     map<text,text>
   );
   
   CREATE TABLE IF NOT EXISTS delta_1_5.all_persistence_ids(
      persistence_id text PRIMARY KEY
   );
   
   CREATE TABLE IF NOT EXISTS delta_1_5_snapshot.snapshots(
      persistence_id    text,
      sequence_nr       bigint,
      timestamp         bigint,
      ser_id            int,
      ser_manifest      text,
      snapshot_data     blob,
      snapshot          blob,
      meta_ser_id       int,
      meta_ser_manifest text,
      meta              blob,
      PRIMARY KEY (persistence_id, sequence_nr))
      WITH CLUSTERING ORDER BY (sequence_nr DESC) AND gc_grace_seconds =864000
      AND compression = {'class': 'LZ4Compressor'}
      AND compaction = {
         'class' : 'SizeTieredCompactionStrategy',
         'enabled' : true,
         'tombstone_compaction_interval' : 86400,
         'tombstone_threshold' : 0.2,
         'unchecked_tombstone_compaction' : false,
         'bucket_high' : 1.5,
         'bucket_low' : 0.5,
         'max_threshold' : 32,
         'min_threshold' : 4,
         'min_sstable_size' : 50
      };
   
      CREATE TABLE IF NOT EXISTS delta_1_5.projections_progress(
         projection_id text primary key,
         offset        timeuuid,
         timestamp     bigint,
         processed     bigint,
         discarded     bigint,
         warnings      bigint,
         failed        bigint,
         value         text
      );
   
   CREATE TABLE IF NOT EXISTS delta_1_5.projections_errors(
      projection_id  text,
      offset         timeuuid,
      timestamp      bigint,
      persistence_id text,
      sequence_nr    bigint,
      value          text,
      severity       text,
      error_type     text,
      message        text,
      PRIMARY KEY ((projection_id), timestamp, persistence_id, sequence_nr))
      WITH CLUSTERING ORDER BY (timestamp ASC, persistence_id ASC, sequence_nr ASC)
      AND compression = {'class': 'LZ4Compressor'};
   ```
Note that @link:[compression](https://cassandra.apache.org/doc/3.11.8/operating/compression.html){ open=new } has been added in v1.5 as the stored events rely more on JSON.
   
4. Count the number of rows in the messages table and in the materialized views to be sure they contain a similar number of rows:
These operations may take a while to complete but they give some insight what can be expected for the migration
* On the messages table:
```cqlsh -e "copy delta.ordered_messages (timebucket) to '/dev/null'" | sed -n 5p | sed 's/ .*//'```
* On the materialized view:
```cqlsh -e 'select timebucket from delta.ordered_messages' | grep rows```
  
When the materialized view has the same number or more rows than the table then it is ready for migration:

4. Pull the docker image for Delta 1.5 from @link:[Docker Hub](https://hub.docker.com/r/bluebrain/nexus-delta)

5. Run Delta in migration mode providing the following parameters when the materialized view has been built:

* Environment variables:
   * KAMON_ENABLED: false
   * MIGRATE_DATA: true
   * DELTA_PLUGINS: /opt/docker/plugins/

* JVM Parameters:
   * Adopt the same values as the one for your current deployment

* Java Properties

| Description                                             | Property                                                        | Example value                                                                  |
|---------------------------------------------------------|-----------------------------------------------------------------|--------------------------------------------------------------------------------|
|Service binding interface                                | app.http.interface                                              | 0.0.0.0                                                                        |
|Service Uri Path prefix                                  | app.http.base-uri                                               | {delta-v5}:8080/v1                                                      |
|Bucket of the first event                                | akka.persistence.cassandra.events-by-tag.first-time-bucket      | YYYYMMDDTHH:MM (It should be the same as the one on your Delta 1.4 deployment) |
|Number of messages to pull from Cassandra from one batch | akka.persistence.cassandra.events-by-tag.max-message-batch-size | 1000                                                                           |
|Cassandra contact point (1)                              | app.database.cassandra.contact-points.1                         | cassandra-1:9042                                                               |
|Cassandra contact point (2)                              | app.database.cassandra.contact-points.2                         | cassandra-2:9042                                                               |
|Cassandra contact point (3)                              | app.database.cassandra.contact-points.3                         | cassandra-3:9042                                                               |
|Cassandra username                                       | app.database.cassandra.username                                 |                                                                                |
|Cassandra password                                       | app.database.cassandra.password                                 |                                                                                |
|The directory for distributed cache to be persisted      | akka.cluster.distributed-data.durable.lmdb.dir                  |                                                                                |
//TODO Complete list

6. Control delta logs at startup to make sure the configuration is right
* No error should be thrown
* The following message indicating migration should appear:
```
2021-02-11 13:23:07 INFO  ch.epfl.bluebrain.nexus.delta.Main - Starting Delta in migration mode
```
* The different plugins should be properly discovered:
```
2021-02-11 13:23:06 INFO  ch.epfl.bluebrain.nexus.delta.Main - Plugins discovered: ... //TODO Add plugin list
```
Keep looking regularly in the log to look if the migration is still ongoing without problems
7. Check the projection progress:
Run the following query on table projections_progress in cqlsh (or your favorite cql client):
```
select processed, failed, warnings, toTimestamp(offset) as migratedUpTo from delta_1_5.projections_progress;
```
This will return a similar response:

```
 processed | failed | warnings | migratedupto
-----------+--------+----------+---------------------------------
     38499 |   1520 |       12 | 2019-04-10 08:53:01.482000+0000
```  

The table contains the following information:
* processed:      total number of processed events
* failed:         number of failures risen during the migration
* warnings:       the current offset
* migratedUpTo:   timestamp of the last event processed by migration when migration last saved progress


When Migration terminates, the number of processed events should be equal to the number of events in your Delta 1.4 deployment.

The errors are reporting in the projections_errors table.
There are two types of errors:
* Failures: The event could not be migrated for the given reason //TODO Add the list of common errors and how to fix them
* Warning : The event could be migrated but a fix had to be performed //TODO Add the list of possible warnings

The table contains the following information:
* projection_id:  the id of the projection (migration-v1.5)
* timestamp:      when the error was risen   
* persistence_id: the internal id of the event in v1.4
* sequence_nr:    the sequence number of the event in v1.4
* offset:         the offset of the event in v1.4
* error_type:     the type of error
* message:        the message and the context of the error                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
* severity:       the level of error (Failure or Warning as explaining above)
* value:          the original json payload of the event in v1.4

To get the errors:
```
select * from delta_1_5.projections_errors limit 100;
```
Note that Cassandra allows you to export table as a CSV file which could be useful if you want to share, process or filter them.
```
copy delta_1_5.projections_errors to '/path/file.csv'
```