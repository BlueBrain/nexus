{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Data Pipeline for a Recommender System using Blue Brain Nexus \n",
    "\n",
    "In this notebook, you will create a recommendation engine using Blue Brain Nexus. Using some movie rating data,\n",
    "you will train a collaborative filtering model for movie recommendation using a given matrix factorization class and export the trained model to Elasticsearch. Once exported, \n",
    "you can test your recommendations by querying Elasticsearch and displaying the results.\n",
    "\n",
    "![Movie Recommendation](assets/recommendation.png)\n",
    "\n",
    "### _Prerequisites_\n",
    "\n",
    "The notebook assumes you have installed [Nexus SDK](https://github.com/BlueBrain/nexus-python-sdk).\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "You will work through the following steps\n",
    "\n",
    "1. Set up the Nexus environment in Python \n",
    "2. Pull the data from Nexus\n",
    "3. Prepare the data\n",
    "4. Train the recommendation model\n",
    "5. Push the model results to Nexus\n",
    "6. Show recommendations by querying Nexus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up Nexus environment\n",
    "\n",
    "Blue Brain Nexus provides a Python SDK to facilitate the use of Nexus, including functionalities of authentication and data access, etc. The SDK is available at https://github.com/BlueBrain/nexus-python-sdk.\n",
    "\n",
    "Here, we assume you have installed the Nexus python SDK. Otherwise, you could pip install the sdk. Then, we will set up the Nexus environment in this notebook with a provided access token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/BlueBrain/nexus-python-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the SDK to ingest and access data in Nexus, you should set up your SDK environment.\n",
    "\n",
    "First, please get your access token through Github.\n",
    "\n",
    "Then, set the access token and your Nexus deployment endpoint in your SDK configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nexussdk as nexus\n",
    "\n",
    "deployment = 'https://nexus-sandbox.io/v1'\n",
    "token = 'YOUR_ACCESS_TOKEN'\n",
    "\n",
    "nexus.config.set_environment(deployment)\n",
    "nexus.config.set_token(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try listing the organization to see if the access is okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('@context',\n",
       "              ['https://bluebrain.github.io/nexus/contexts/admin.json',\n",
       "               'https://bluebrain.github.io/nexus/contexts/resource.json',\n",
       "               'https://bluebrain.github.io/nexus/contexts/search.json']),\n",
       "             ('_total', 3),\n",
       "             ('_results',\n",
       "              [OrderedDict([('@id', 'https://nexus-sandbox.io/v1/orgs/amld'),\n",
       "                            ('@type', 'Organization'),\n",
       "                            ('description', 'AMLD Workshop'),\n",
       "                            ('_uuid', '2b8df7c8-238f-476d-8a07-5fbb6c92fa3d'),\n",
       "                            ('_label', 'amld'),\n",
       "                            ('_rev', 1),\n",
       "                            ('_deprecated', False),\n",
       "                            ('_createdAt', '2019-01-26T20:38:48.311Z'),\n",
       "                            ('_createdBy',\n",
       "                             'https://nexus-sandbox.io/v1/realms/github/users/bogdanromanx'),\n",
       "                            ('_updatedAt', '2019-01-26T20:38:48.311Z'),\n",
       "                            ('_updatedBy',\n",
       "                             'https://nexus-sandbox.io/v1/realms/github/users/bogdanromanx')]),\n",
       "               OrderedDict([('@id', 'https://nexus-sandbox.io/v1/orgs/broman'),\n",
       "                            ('@type', 'Organization'),\n",
       "                            ('description', \"Bogdan Roman's Organization\"),\n",
       "                            ('_uuid', 'a8266f8b-17c4-4f25-a5b3-340e4cfdc492'),\n",
       "                            ('_label', 'broman'),\n",
       "                            ('_rev', 1),\n",
       "                            ('_deprecated', False),\n",
       "                            ('_createdAt', '2019-01-26T20:49:10.477Z'),\n",
       "                            ('_createdBy',\n",
       "                             'https://nexus-sandbox.io/v1/realms/github/users/bogdanromanx'),\n",
       "                            ('_updatedAt', '2019-01-26T20:49:10.477Z'),\n",
       "                            ('_updatedBy',\n",
       "                             'https://nexus-sandbox.io/v1/realms/github/users/bogdanromanx')]),\n",
       "               OrderedDict([('@id',\n",
       "                             'https://nexus-sandbox.io/v1/orgs/tutorialnexus'),\n",
       "                            ('@type', 'Organization'),\n",
       "                            ('description', 'Nexus Tutorial'),\n",
       "                            ('_uuid', '136e78e0-5d27-4d4a-baa1-c75ebc711b20'),\n",
       "                            ('_label', 'tutorialnexus'),\n",
       "                            ('_rev', 2),\n",
       "                            ('_deprecated', False),\n",
       "                            ('_createdAt', '2019-01-26T21:07:32.259Z'),\n",
       "                            ('_createdBy',\n",
       "                             'https://nexus-sandbox.io/v1/realms/github/users/huanxiang'),\n",
       "                            ('_updatedAt', '2019-01-26T21:09:21.315Z'),\n",
       "                            ('_updatedBy',\n",
       "                             'https://nexus-sandbox.io/v1/realms/github/users/bogdanromanx')])])])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nexus.organizations.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pull data from Nexus\n",
    "\n",
    "Now we will start pulling data that has been ingested into your Nexus project previously. \n",
    "\n",
    "For building a classical recommendation system using matrix factorization, we will need a user-by-item matrix where nonzero elements of the matrix are ratings that a user has given an item. To do that, we will \n",
    "\n",
    "1.   Query all the rating data for building a U-I matrix\n",
    "2.   Query the movie id data for the recomendation\n",
    "\n",
    "SPARQL is an RDF query language which is able to retrieve and manipulate data stored in Resource Description Framework (RDF) format. Given that all the movielens data is put into the knowledge graph meaning they are inter-connected, it is straightforward to query the data using SPARQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment for SPARQL\n",
    "We will first need to install a python wrapper around a SPARQL service. It helps in creating the query URI and, possibly, convert the result into a more manageable format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/RDFLib/sparqlwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import repeat\n",
    "import concurrent.futures\n",
    "import json\n",
    "import csv\n",
    "import math\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy.linalg import solve\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from urllib.parse import urlencode, quote_plus\n",
    "from collections import OrderedDict\n",
    "from io import StringIO\n",
    "from functools import reduce\n",
    "from SPARQLWrapper import SPARQLWrapper, XML, N3, TURTLE, JSON, POST, POSTDIRECTLY, CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now set up the SPARQL client based on the Nexus setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.path.join(deployment, \"views/amld/recommender/graph/sparql/\")\n",
    "headers = {}\n",
    "headers[\"Authorization\"] =\"Bearer {}\".format(token)\n",
    "headers[\"Content-Type\"] =\"application/sparql-query\"\n",
    "\n",
    "sparql_client = SPARQLWrapper(endpoint)\n",
    "sparql_client.addCustomHttpHeader(\"Content-Type\", \"application/sparql-query\")\n",
    "sparql_client.addCustomHttpHeader(\"Authorization\",\"Bearer {}\".format(token))\n",
    "sparql_client.setMethod(POST)\n",
    "sparql_client.setReturnFormat(JSON)\n",
    "sparql_client.setRequestMethod(POSTDIRECTLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define some helper function that can convert the JSON payload from the SPARQL client to DataFrame, which can be later used to build the U-I matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SPARQL results into a Pandas data frame\n",
    "def sparql2dataframe(json_sparql_results, df):\n",
    "    cols = json_sparql_results['head']['vars']\n",
    "    out = []\n",
    "    for row in json_sparql_results['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "        out.append(item)\n",
    "    df_temp = pd.DataFrame(out, columns=cols)\n",
    "    return pd.concat([df, df_temp])\n",
    "\n",
    "#Use a client and send a query\n",
    "def query(query, sparql_client):\n",
    "    sparql_client.setQuery(query)\n",
    "    result_object = sparql_client.query()\n",
    "    return result_object._convertJSON()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the rating data from Nexus\n",
    "\n",
    "Here we define a function that will first perform a query to the ElasticSearch view in Nexus to fetch the number of the total rating data, which is about 100k in the case of a small Movielens data. Then, we create a SPARQL query that will fill in a dataframe with the fields of ['userId', 'movieId', 'rating']. We perform this query with a limit of 5000 entries per batch and loop over through the 100k data. This is to avoid overloading a large HTTP response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and run query\n",
    "def load_rating_from_nexus(batch_size=5000):\n",
    "    \n",
    "    es_query = {\n",
    "        \"query\": {\n",
    "        \"terms\" : {\"@type\":[\"https://nexus-sandbox.io/v1/vocabs/amld/recommender/Rating\"]}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    es_payload = nexus.views.query_es(org_label=\"amld\", project_label=\"recommender\", \\\n",
    "                                      view_id='documents', query=es_query)\n",
    "    total_items = es_payload['hits']['total']\n",
    "        \n",
    "    batches = math.ceil(float(total_items)/batch_size)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['userId', 'movieId', 'rating'])\n",
    "    \n",
    "    for i in range(batches): \n",
    "        start_idx = i * batch_size\n",
    "        if i == batches - 1:\n",
    "            size = total_items % batch_size\n",
    "        else:\n",
    "            size = batch_size\n",
    "            \n",
    "        sparql_query = \"\"\"\n",
    "        PREFIX vocab: <https://nexus-sandbox.io/v1/vocabs/amld/recommender/>\n",
    "\n",
    "        Select ?userId ?movieId ?rating \n",
    "         WHERE  {\n",
    "            ?ratingNode a vocab:Rating.\n",
    "            ?ratingNode vocab:movieId ?movieId.\n",
    "            ?ratingNode vocab:rating ?rating.\n",
    "            ?ratingNode vocab:userId ?userId.\n",
    "        }\n",
    "        ORDER BY ASC(?userId) ASC (?movieId)\n",
    "        OFFSET %start_idx%\n",
    "        LIMIT %batch_size%\"\"\".replace('%start_idx%', str(start_idx)).replace('%batch_size%', str(size))\n",
    "                \n",
    "        results = query(sparql_query, sparql_client)\n",
    "        df = sparql2dataframe(results, df)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will start loading the data. Depending on the infrastructure of the Nexus deployment, this might take up to a few minutes. After that, we will verify the shape of the data is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating = load_rating_from_nexus()\n",
    "df_rating.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will retrieve the movie id information using the same way. The retrieved data will be store in a dataframe with fields ['movieId', 'title']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and run query\n",
    "def load_movie_from_nexus(batch_size=5000):\n",
    "    \n",
    "    es_query = {\n",
    "        \"query\": {\n",
    "        \"terms\" : {\"@type\":[\"https://nexus-sandbox.io/v1/vocabs/amld/recommender/Movie\"]}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    es_payload = nexus.views.query_es(org_label=\"amld\", project_label=\"recommender\", \\\n",
    "                                      view_id='documents', query=es_query)\n",
    "    total_items = es_payload['hits']['total']\n",
    "        \n",
    "    batches = math.ceil(float(total_items)/batch_size)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['movieId', 'title'])\n",
    "    \n",
    "    for i in range(batches): \n",
    "        start_idx = i * batch_size\n",
    "        if i == batches - 1:\n",
    "            size = total_items % batch_size\n",
    "        else:\n",
    "            size = batch_size\n",
    "            \n",
    "        sparql_query = \"\"\"\n",
    "        PREFIX vocab: <https://nexus-sandbox.io/v1/vocabs/amld/recommender/>\n",
    "\n",
    "        Select ?movieId  ?title\n",
    "         WHERE  {\n",
    "            ?movieNode a vocab:Movie.\n",
    "            ?movieNode vocab:movieId ?movieId.\n",
    "            ?movieNode vocab:title ?title.\n",
    "\n",
    "        }\n",
    "        ORDER BY ASC(?movieId) ASC (?title)\n",
    "        OFFSET %start_idx%\n",
    "        LIMIT %batch_size%\"\"\".replace('%start_idx%', str(start_idx)).replace('%batch_size%', str(size))\n",
    "        \n",
    "        results = query(sparql_query, sparql_client)\n",
    "        df = sparql2dataframe(results, df)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8633, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie = load_movie_from_nexus()\n",
    "df_movie.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare the data\n",
    "\n",
    "To get the recommendation right, we must construct and transform the data correctly. This is usually a very important step so that you are sure your machine learning algorithm is consuming the correct data in a good way.\n",
    "\n",
    "In the case of collaborative filtering using matrix factorization, the preparation of the data contains the following steps:\n",
    "\n",
    "- As in the U-I matrix we will have the user id and the item id as incremental integers, we will assign a unique number between (0, #users) to each user and do the same for movies. The mapping between the id in the U-I matrix will be stored, which can be further used to query the recommendation. \n",
    "\n",
    "\n",
    "- Then, we will create the U-I matrix by assigning each user's rating to each movie on a zero matrix created using numpy.\n",
    "\n",
    "\n",
    "- Finally, we will split the data into training and testing. This is done by removing 10 ratings for each user and assign them to the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_mapping = dict( enumerate(df_rating.movieId.astype('category').cat.categories) )\n",
    "inv_movie_mapping = {v: k for k, v in movie_mapping.items()}\n",
    "\n",
    "df_rating.userId = df_rating.userId.astype('category').cat.codes.values\n",
    "df_rating.movieId = df_rating.movieId.astype('category').cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df_rating.userId.unique().shape[0]\n",
    "n_items = df_rating.movieId.unique().shape[0]\n",
    "\n",
    "# Create r_{ui}, our ratings matrix\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df_rating.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to provide a map from movie id to title for the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx_to_movie = {}\n",
    "movie_list = df_movie.movieId.unique()\n",
    "for k, v in movie_mapping.items():\n",
    "    movie_v = str(int(float(v)))\n",
    "    if movie_v in movie_list:\n",
    "        idx_to_movie[k] = df_movie[df_movie.movieId==movie_v].title.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train a recommmender model on the ratings data\n",
    "\n",
    "Your data is now prepared as a U-I matrix and you will use it to build a collaborative filtering recommendation model.\n",
    "\n",
    "Collaborative filtering is a recommendation approach that is effectively based on the \"wisdom of the crowd\". It makes the assumption that, if two people share similar preferences, then the things that one of them prefers could be good recommendations to make to the other. In other words, if user A tends to like certain movies, and user B shares some of these preferences with user A, then the movies that user A likes, that user B has not yet seen, may well be movies that user B will also like.\n",
    "\n",
    "In a similar manner, we can think about items as being similar if they tend to be rated highly by the same people, on average.\n",
    "\n",
    "Hence these models are based on the combined, collaborative preferences and behavior of all users in aggregate. They tend to be very effective in practice (provided you have enough preference data to train the model). The ratings data you have is a form of explicit preference data, perfect for training collaborative filtering models. \n",
    "\n",
    "Matrix factorization (MF) is a classical method to perform collaborative filtering model. The core idea of MF is to represent the ratings as a user-item ratings matrix. In the diagram below you will see this matrix on the left (with users as rows and movies as columns). The entries in this matrix are the ratings given by users to movies. \n",
    "\n",
    "You may also notice that the matrix has missing entries because not all users have rated all movies. In this situation we refer to the data as sparse.\n",
    "\n",
    "![](assets/collaborative_filtering.png)\n",
    "\n",
    "MF methods aim to find two much smaller matrices (one representing the users and the other the items) that, when multiplied together, re-construct the original ratings matrix as closely as possible. This is know as factorizing the original matrix, hence the name of the technique.\n",
    "\n",
    "The two smaller matrices are called factor matrices (or latent features). The user and movie factor matrices are illustrated on the right in the diagram above. The idea is that each user factor vector is a compressed representation of the user's preferences and behavior. Likewise, each item factor vector is a compressed representation of the item. Once the model is trained, the factor vectors can be used to make recommendations, which is what you will do in the following sections.\n",
    "\n",
    "The optimization of the MF can be done using different methods. In this example, we will use 2 popular methods:\n",
    "\n",
    "- Alternating Least Squares (ALS)\n",
    "\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "\n",
    "\n",
    "Further reading:\n",
    "\n",
    "[Explicit Matrix Factorization: ALS, SGD, and All That Jazz](https://www.ethanrosenthal.com/2016/01/09/explicit-matrix-factorization-sgd-als/)\n",
    "\n",
    "[ALS Implicit Collaborative Filtering\n",
    "](https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have provided an explicit MF class adapted from the example of an amazing [tutorial](https://www.ethanrosenthal.com/2016/01/09/explicit-matrix-factorization-sgd-als/) by Ethan Rosenthal, which can perform MF with both ALS and SGD optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMF():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 n_factors=40,\n",
    "                 learning='sgd',\n",
    "                 item_fact_reg=0.0, \n",
    "                 user_fact_reg=0.0,\n",
    "                 item_bias_reg=0.0,\n",
    "                 user_bias_reg=0.0,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        learning : (str)\n",
    "            Method of optimization. Options include \n",
    "            'sgd' or 'als'.\n",
    "        \n",
    "        item_fact_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_fact_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg : (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg : (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.learning = learning\n",
    "        if self.learning == 'sgd':\n",
    "            self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "            self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10, learning_rate=0.1):\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors        \n",
    "        self.user_vecs = np.random.normal(scale=1./self.n_factors,\\\n",
    "                                          size=(self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.n_factors,\n",
    "                                          size=(self.n_items, self.n_factors))\n",
    "        \n",
    "        if self.learning == 'als':\n",
    "            self.partial_train(n_iter)\n",
    "        elif self.learning == 'sgd':\n",
    "            self.learning_rate = learning_rate\n",
    "            self.user_bias = np.zeros(self.n_users)\n",
    "            self.item_bias = np.zeros(self.n_items)\n",
    "            self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "            self.partial_train(n_iter)\n",
    "    \n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 10 == 0 and self._v:\n",
    "                print ('\\tcurrent iteration: {}'.format(ctr))\n",
    "            if self.learning == 'als':\n",
    "                self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                               self.item_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.user_fact_reg, \n",
    "                                               type='user')\n",
    "                self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                               self.user_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.item_fact_reg, \n",
    "                                               type='item')\n",
    "            elif self.learning == 'sgd':\n",
    "                self.training_indices = np.arange(self.n_samples)\n",
    "                np.random.shuffle(self.training_indices)\n",
    "                self.sgd()\n",
    "            ctr += 1\n",
    "\n",
    "    def sgd(self):\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            prediction = self.predict(u, i)\n",
    "            e = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias[u] += self.learning_rate * \\\n",
    "                                (e - self.user_bias_reg * self.user_bias[u])\n",
    "            self.item_bias[i] += self.learning_rate * \\\n",
    "                                (e - self.item_bias_reg * self.item_bias[i])\n",
    "            \n",
    "            #Update latent factors\n",
    "            self.user_vecs[u, :] += self.learning_rate * \\\n",
    "                                    (e * self.item_vecs[i, :] - \\\n",
    "                                     self.user_fact_reg * self.user_vecs[u,:])\n",
    "            self.item_vecs[i, :] += self.learning_rate * \\\n",
    "                                    (e * self.user_vecs[u, :] - \\\n",
    "                                     self.item_fact_reg * self.item_vecs[i,:])\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        if self.learning == 'als':\n",
    "            return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        elif self.learning == 'sgd':\n",
    "            prediction = self.global_bias + self.user_bias[u] + self.item_bias[i]\n",
    "            prediction += self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "            return prediction\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item.\"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print ('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff, learning_rate)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print ('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print ('Test mse: ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first train an ALS model. The idea of ALS is that, we first hold one set of latent vectors constant. We then take the derivative of the loss function with respect to the other set of vectors. We set the derivative equal to zero  and solve for the non-constant vectors. With these new, solved-for user vectors in hand, we hold them constant, instead, and take the derivative of the loss function with respect to the previously constant vectors (the item vectors). We alternate back and forth and carry out this two-step dance until convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model = ExplicitMF(ratings, n_factors=20, learning='als', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01)\n",
    "als_model.train(50)\n",
    "\n",
    "user_vec_als = als_model.user_vecs\n",
    "movie_vec_als = als_model.item_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model is the SGD model. The idea is also to take derivatives of the loss function. But instead we take the derivative with respect to each variable in the model. The “stochastic” aspect of the algorithm involves taking the derivative and updating feature weights one individual sample at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model = ExplicitMF(ratings, n_factors=40, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01)\n",
    "sgd_model.train(20, learning_rate=0.001)\n",
    "\n",
    "user_vec_sgd = sgd_model.user_vecs\n",
    "movie_vec_sgd = sgd_model.item_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the embedding matrices into local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sgd_vec.npy', movie_vec_sgd.astype(np.float16))\n",
    "np.save('als_vec.npy', movie_vec_als.astype(np.float16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Push the embedding matrix to Nexus\n",
    "\n",
    "Now that we have the models trained, we can now push the embedding matrices back to Nexus. \n",
    "\n",
    "First, we will create a file that stores the movie embedding matrix of the SGD model in Nexus by using the SDK. We then keep the @id of the created file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_org = 'YOUR_ORG'\n",
    "your_proj = 'YOUR_PROJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = nexus.files.create(org_label=your_org, project_label=your_proj, filepath='als_vec.npy')\n",
    "als_vec_file_id = r['@id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a resource of this data linking to the previously pushed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_embedding_payload = {\n",
    "    'modelName': 'ALS',\n",
    "    'fileId': als_vec_file_id\n",
    "}\n",
    "r = nexus.resources.create(org_label=your_org, project_label=your_proj, data=als_embedding_payload)\n",
    "als_vec_res_id = r['@id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Show recommendation by querying Nexus\n",
    "\n",
    "Now that you have loaded your recommendation model into Nexus, we will perform some recommendation by querying the models stored in Nexus.\n",
    "\n",
    "* fetch the embedding matrix stored in Nexus\n",
    "* compute the similarity matrix of the movie\n",
    "* display top k similar movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use the Nexus id of the embedding matrix to fetch the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = nexus.files.fetch(org_label=your_org, project_label=your_proj, \\\n",
    "                      file_id = als_vec_file_id, out_filepath='./als_vec.npy')\n",
    "embedding_mat = np.load('./als_vec.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the similarity matrix of the movies for both two outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vecs):\n",
    "    sim = vecs.dot(vecs.T)\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return sim / norms / norms.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = cosine_similarity(embedding_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to retrieve top k movies which are similar to a given movie Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top_k_movies_name(similarity, mapper, movie_idx, k=5):\n",
    "    print('The recommended films for user who likes \"%s\"' % (mapper[movie_idx]))\n",
    "    \n",
    "    movie_indices = np.argsort(similarity[movie_idx,:])[::-1]\n",
    "    images = ''\n",
    "    k_ctr = 0\n",
    "    # Start i at 1 to not grab the input movie\n",
    "    i = 1\n",
    "    while k_ctr < 5:\n",
    "        if movie_indices[i] in mapper.keys():\n",
    "            movie = mapper[movie_indices[i]]\n",
    "            print(' - ' + movie)\n",
    "            k_ctr += 1\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recommended films for user who likes \"Movie 43 (2013)\"\n",
      " - The Hungover Games (2014)\n",
      " - Charlie's Angels: Full Throttle (2003)\n",
      " - Behind the Candelabra (2013)\n",
      " - Best Defense (1984)\n",
      " - Crows Zero (Kurôzu zero) (2007)\n"
     ]
    }
   ],
   "source": [
    "movie_id = 5 # Heat\n",
    "display_top_k_movies_name(sim, idx_to_movie, movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
